# ğŸ—ï¸ SYSTEM ARCHITECTURE - Fake News Detection System# ğŸ¯ FAKE NEWS DETECTION SYSTEM - COMPLETE ARCHITECTURE OVERVIEW



## ğŸ“‹ Table of Contents## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)

1. [System Overview](#system-overview)2. [Three-Agent Architecture](#three-agent-architecture)

2. [Three-Agent Architecture](#three-agent-architecture)3. [Data Flow](#data-flow)

3. [Data Flow](#data-flow)4. [Integration Points](#integration-points)

4. [Technology Stack](#technology-stack)5. [API Endpoints](#api-endpoints)

5. [API Endpoints](#api-endpoints)6. [Technology Stack](#technology-stack)

6. [Integration Points](#integration-points)7. [Key Features](#key-features)

7. [Security & Performance](#security--performance)8. [Deployment](#deployment)

8. [Deployment Guide](#deployment-guide)

9. [File Structure](#file-structure)---

10. [Configuration](#configuration)

## ğŸ¯ System Overview

---

The **Fake News Detection System** is a sophisticated multi-agent AI platform that analyzes news articles and text content to determine credibility. It uses a three-agent architecture with ensemble machine learning models and real-time news verification.

## 1. System Overview

### Key Statistics:

The **Fake News Detection System** is a sophisticated Flask-based web application that uses a **three-agent architecture** to analyze news articles and determine their credibility. The system combines machine learning models, external news verification, and intelligent consensus analysis to provide accurate fake news detection.- **3 AI Agents** working in harmony

- **3 ML Models** (SVM, LSTM, BERT) with 94% ensemble accuracy

### Key Features- **Real-time** news fetching from NewsAPI

- **Comprehensive** credibility analysis

âœ… **Three-Agent Intelligence**:- **Human-readable** explanations

- **Agent 1**: Article Collector - Fetches and preprocesses news

- **Agent 2**: Credibility Analyzer - ML-based analysis with ensemble voting---

- **Agent 3**: Verdict Agent - Generates final verdict with explanations

## ğŸ—ï¸ Three-Agent Architecture

âœ… **Multi-Model Ensemble**:

- SVM (Support Vector Machine) - 99.5% accuracyThe system is built around three specialized agents, each with distinct responsibilities:

- LSTM (Long Short-Term Memory) - 87.0% accuracy

- BERT (Bidirectional Encoder Representations) - 89.0% accuracy```

- **Ensemble Accuracy**: ~94% with majority votingâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                    USER INPUT                                â”‚

âœ… **External Verification**:â”‚  (Text to analyze / URL to check / News to fetch)           â”‚

- NewsAPI.org integration for real-time fact-checkingâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- Semantic similarity matching                   â”‚

- Cross-reference validation                   â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

âœ… **Dual Operation Modes**:â”‚              AGENT 1: ARTICLE COLLECTOR                      â”‚

- **Free Mode**: Basic analysis and URL verificationâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚

- **Commercial Mode**: Advanced features, subscriptions, usage trackingâ”‚  â”‚  â€¢ Fetch from NewsAPI                              â”‚     â”‚

â”‚  â”‚  â€¢ Extract content from URLs                       â”‚     â”‚

### System Capabilitiesâ”‚  â”‚  â€¢ Deduplicate articles                            â”‚     â”‚

â”‚  â”‚  â€¢ Preprocess text                                 â”‚     â”‚

- ğŸ“° **News Fetching**: Retrieve articles from NewsAPI by country/categoryâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚

- ğŸ”— **URL Analysis**: Extract and analyze content from any URLâ”‚  Files: news_fetcher.py, news_apis/newsapi_client.py        â”‚

- ğŸ¤– **ML Prediction**: Three-model ensemble with confidence scoringâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- âœ… **Verification**: Cross-check against trusted news sources                   â”‚

- ğŸ“Š **Analytics**: Track analysis history and statistics                   â–¼

- ğŸ’¼ **Commercial**: Subscription plans, API limits, user managementâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚           AGENT 2: CREDIBILITY ANALYZER                      â”‚

---â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚

â”‚  â”‚  ML Models:                                        â”‚     â”‚

## 2. Three-Agent Architectureâ”‚  â”‚  â€¢ SVM (99.5% accuracy)                           â”‚     â”‚

â”‚  â”‚  â€¢ LSTM (87.0% accuracy)                          â”‚     â”‚

The system uses a **sequential three-agent pipeline** where each agent has specialized responsibilities:â”‚  â”‚  â€¢ BERT (89.0% accuracy)                          â”‚     â”‚

â”‚  â”‚                                                    â”‚     â”‚

```â”‚  â”‚  â€¢ Ensemble voting (Majority)                     â”‚     â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚  â€¢ Feature extraction                             â”‚     â”‚

â”‚                    USER REQUEST                                  â”‚â”‚  â”‚  â€¢ Confidence calibration                         â”‚     â”‚

â”‚              (Text / URL / News Query)                          â”‚â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  Files: utils/model_loader.py, utils/predictor.py           â”‚

                         â”‚â”‚         credibility_analyzer/credibility_analyzer.py         â”‚

                         â–¼â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚

â”‚                  AGENT 1: ARTICLE COLLECTOR                      â”‚                   â–¼

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  â”‚  â€¢ Fetch news from NewsAPI                                â”‚  â”‚â”‚             AGENT 3: VERDICT AGENT                           â”‚

â”‚  â”‚  â€¢ Extract content from URLs                              â”‚  â”‚â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚

â”‚  â”‚  â€¢ Preprocess and clean text                              â”‚  â”‚â”‚  â”‚  â€¢ Analyze model consensus                         â”‚     â”‚

â”‚  â”‚  â€¢ Deduplicate articles                                   â”‚  â”‚â”‚  â”‚  â€¢ Calculate final confidence                      â”‚     â”‚

â”‚  â”‚  â€¢ Prepare for analysis                                   â”‚  â”‚â”‚  â”‚  â€¢ Generate explanations                           â”‚     â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚  â”‚  â€¢ NewsAPI cross-verification                      â”‚     â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚  â€¢ Final verdict synthesis                         â”‚     â”‚

                         â”‚â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚

                         â”‚ Clean Textâ”‚  Files: verdict_agent/verdict_agent.py, app.py              â”‚

                         â–¼â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚

â”‚               AGENT 2: CREDIBILITY ANALYZER                      â”‚                   â–¼

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  â”‚  ML MODEL 1: SVM                                          â”‚  â”‚â”‚                    OUTPUT                                     â”‚

â”‚  â”‚    â†’ Prediction: FAKE/TRUE                                â”‚  â”‚â”‚  {                                                            â”‚

â”‚  â”‚    â†’ Confidence: 99.5%                                    â”‚  â”‚â”‚    "verdict": "TRUE",                                         â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚    "confidence": 91.0,                                        â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚    "explanation": "High probability of credible content...",  â”‚

â”‚  â”‚  ML MODEL 2: LSTM                                         â”‚  â”‚â”‚    "individual_results": {...},                               â”‚

â”‚  â”‚    â†’ Prediction: FAKE/TRUE                                â”‚  â”‚â”‚    "news_verification": {...}                                 â”‚

â”‚  â”‚    â†’ Confidence: 87.0%                                    â”‚  â”‚â”‚  }                                                            â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚```

â”‚  â”‚  ML MODEL 3: BERT                                         â”‚  â”‚

â”‚  â”‚    â†’ Prediction: FAKE/TRUE                                â”‚  â”‚---

â”‚  â”‚    â†’ Confidence: 89.0%                                    â”‚  â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚## ğŸ“Š Agent Responsibilities

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚

â”‚  â”‚  ENSEMBLE VOTING                                          â”‚  â”‚### **AGENT 1: Article Collector** ğŸ“°

â”‚  â”‚    â†’ Majority Vote: 2 out of 3 models                     â”‚  â”‚**Role**: Data Acquisition and Preprocessing

â”‚  â”‚    â†’ Confidence Weighting                                 â”‚  â”‚

â”‚  â”‚    â†’ Final Prediction                                     â”‚  â”‚**Responsibilities**:

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚1. Fetch news from NewsAPI (top headlines, search)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜2. Extract content from URLs (web scraping)

                         â”‚3. Deduplicate articles (hash-based)

                         â”‚ Model Predictions4. Clean and format text

                         â–¼5. Prepare data for analysis

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                  AGENT 3: VERDICT AGENT                          â”‚**Input**: 

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚- NewsAPI credentials

â”‚  â”‚  â€¢ Analyze model consensus                                â”‚  â”‚- Search parameters (country, category, query)

â”‚  â”‚  â€¢ Calculate final confidence                             â”‚  â”‚- URLs to extract

â”‚  â”‚  â€¢ Generate human-readable explanation                    â”‚  â”‚

â”‚  â”‚  â€¢ Integrate NewsAPI verification (optional)              â”‚  â”‚**Output**: 

â”‚  â”‚  â€¢ Determine verdict type:                                â”‚  â”‚- Enriched articles with metadata

â”‚  â”‚    - TRUE (Strong Agreement)                              â”‚  â”‚- Clean text for analysis

â”‚  â”‚    - FALSE (Strong Agreement)                             â”‚  â”‚- Article statistics

â”‚  â”‚    - MISLEADING (Mixed Signals)                           â”‚  â”‚

â”‚  â”‚    - UNCERTAIN (Low Confidence)                           â”‚  â”‚**Key Files**:

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚- `news_fetcher.py` (main orchestrator)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- `news_apis/newsapi_client.py` (API client)

                         â”‚- `app.py` (extract_article_content function)

                         â”‚ Final Verdict

                         â–¼**Code Example**:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```python

â”‚                     JSON RESPONSE                                â”‚from news_fetcher import NewsFetcher

â”‚  {                                                               â”‚

â”‚    "prediction": "TRUE",                                         â”‚fetcher = NewsFetcher()

â”‚    "confidence": 95.5,                                           â”‚articles = fetcher.fetch_and_analyze(

â”‚    "verdict_type": "TRUE",                                       â”‚    country='us',

â”‚    "explanation": "All models agree...",                         â”‚    category='technology',

â”‚    "model_results": [...],                                       â”‚    page_size=10

â”‚    "news_verification": {...}                                    â”‚)

â”‚  }                                                               â”‚# Returns: List of articles with credibility scores

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```

```

---

### Agent Responsibilities

### **AGENT 2: Credibility Analyzer** ğŸ”

#### **Agent 1: Article Collector** (`news_fetcher.py`)**Role**: Machine Learning Prediction

- **Input**: URL, news query, or raw text

- **Process**: Fetch â†’ Extract â†’ Clean â†’ Deduplicate**Responsibilities**:

- **Output**: Clean, preprocessed text ready for analysis1. Load and manage ML models (SVM, LSTM, BERT)

- **Key Files**: `news_fetcher.py`, `news_apis/newsapi_client.py`2. Preprocess text (cleaning, normalization)

3. Extract advanced features (sentiment, readability, etc.)

#### **Agent 2: Credibility Analyzer** (`utils/predictor.py`)4. Run ensemble prediction with majority voting

- **Input**: Clean text from Agent 15. Calculate confidence scores

- **Process**: Load models â†’ Predict (SVM, LSTM, BERT) â†’ Ensemble voting6. Provide individual model results

- **Output**: Prediction, confidence, individual model results

- **Key Files**: `utils/model_loader.py`, `utils/predictor.py`, `credibility_analyzer/`**Input**: 

- Raw text content

#### **Agent 3: Verdict Agent** (`verdict_agent/verdict_agent.py`)- Model files (PKL, H5, pre-trained)

- **Input**: Text + Model predictions from Agent 2

- **Process**: Analyze consensus â†’ Calculate confidence â†’ Generate explanation**Output**: 

- **Output**: Final verdict with human-readable explanation- Final prediction (FAKE/TRUE)

- **Key Files**: `verdict_agent/verdict_agent.py`- Confidence score (0-100%)

- Individual model predictions

---- Voting details



## 3. Data Flow**Key Files**:

- `utils/model_loader.py` (load models)

### Complete Analysis Pipeline- `utils/predictor.py` (ensemble prediction)

- `credibility_analyzer/credibility_analyzer.py` (analysis)

```- `credibility_analyzer/feature_extractor.py` (features)

USER INPUT

    â”‚**Code Example**:

    â”œâ”€â†’ Text Analysis Flow```python

    â”‚   1. User provides text directlyfrom utils.model_loader import ModelLoader

    â”‚   2. Text â†’ Agent 2 (Credibility Analyzer)from utils.predictor import UnifiedPredictor

    â”‚   3. Agent 2 â†’ Agent 3 (Verdict Agent)

    â”‚   4. Return verdict to userloader = ModelLoader()

    â”‚loader.load_all_models()

    â”œâ”€â†’ URL Analysis Flow

    â”‚   1. User provides URLpredictor = UnifiedPredictor(loader)

    â”‚   2. Agent 1 extracts content from URL (BeautifulSoup)result = predictor.ensemble_predict_majority(text)

    â”‚   3. Extracted text â†’ Agent 2 (Credibility Analyzer)# Returns: {

    â”‚   4. Agent 2 â†’ Agent 3 (Verdict Agent)#   "final_prediction": "TRUE",

    â”‚   5. Return verdict to user#   "confidence": 91.0,

    â”‚#   "individual_results": {...}

    â””â”€â†’ News Fetching Flow# }

        1. User requests news (country, category)```

        2. Agent 1 fetches from NewsAPI

        3. Agent 1 deduplicates articles---

        4. For each article:

           a. Agent 1 â†’ Agent 2 (Credibility Analyzer)### **AGENT 3: Verdict Agent** âš–ï¸

           b. Agent 2 â†’ Agent 3 (Verdict Agent)**Role**: Decision Making and Explanation

        5. Return analyzed articles with verdicts

```**Responsibilities**:

1. Analyze consensus among models

### Data Transformation2. Calculate final confidence (weighted by agreement)

3. Generate human-readable explanations

```4. Integrate NewsAPI verification results

Raw Input â†’ Cleaned Text â†’ ML Features â†’ Predictions â†’ Verdict â†’ Explanation5. Provide comprehensive verdicts

6. Format results for display

1. Raw Input (Any format)

   â†“**Input**: 

2. Agent 1: Text Cleaning- Model predictions from Agent 2

   - Remove HTML tags- Original text

   - Strip special characters- NewsAPI verification results

   - Normalize whitespace

   - Extract main content**Output**: 

   â†“- Final verdict (TRUE/FALSE/MISLEADING/UNCERTAIN)

3. Agent 2: Feature Extraction- Confidence score (0.0-1.0)

   - SVM: TF-IDF vectorization- Detailed explanation with emojis

   - LSTM: Tokenization + Padding- Consensus analysis

   - BERT: Contextual embeddings- Recommendations

   â†“

4. Agent 2: Model Predictions**Key Files**:

   - SVM: Linear classification- `verdict_agent/verdict_agent.py` (verdict logic)

   - LSTM: Sequential analysis- `app.py` (generate_explanation function)

   - BERT: Transformer-based classification

   â†“**Code Example**:

5. Agent 2: Ensemble Voting```python

   - Collect all predictionsfrom verdict_agent.verdict_agent import VerdictAgent, ModelResult

   - Apply majority voting

   - Weight by confidenceagent = VerdictAgent()

   â†“model_results = [

6. Agent 3: Consensus Analysis    ModelResult("SVM", "true", 0.995, "traditional_ml", 0.995),

   - Check agreement level    ModelResult("LSTM", "true", 0.823, "deep_learning", 0.870),

   - Map to verdict type    ModelResult("BERT", "true", 0.912, "transformer", 0.890)

   - Calculate final confidence]

   â†“

7. Agent 3: Explanation Generationverdict = agent.generate_verdict(text, model_results)

   - Generate human-readable text# Returns: {

   - Include model reasoning#   "verdict": "true",

   - Add verification results (if available)#   "confidence": 0.91,

   â†“#   "explanation": "The content appears to be TRUE...",

8. JSON Response#   "consensus_analysis": {...}

   - Structured output# }

   - All metadata included```

   - Ready for frontend display

```---



---## ğŸ”„ Complete Data Flow



## 4. Technology Stack### **End-to-End Process**:



### Core Technologies```

1. USER REQUEST

#### **Backend Framework**   â”œâ”€ Text Analysis: User submits text

- **Flask 3.0.0**: Web framework   â”œâ”€ URL Analysis: User submits URL

  - Routes and request handling   â””â”€ News Fetching: User requests news by category

  - Session management

  - Template rendering2. AGENT 1: ARTICLE COLLECTOR

- **Flask-CORS**: Cross-Origin Resource Sharing   â”œâ”€ Fetch from NewsAPI OR Extract from URL

  - Enable API access from frontend   â”œâ”€ Clean and preprocess text

   â”œâ”€ Deduplicate (if multiple articles)

#### **Machine Learning**   â””â”€ Pass to Agent 2

- **scikit-learn 1.4.0**: SVM model

  - TF-IDF vectorization3. AGENT 2: CREDIBILITY ANALYZER

  - Linear SVM classifier   â”œâ”€ Load ML models (SVM, LSTM, BERT)

  - 99.5% accuracy on test set   â”œâ”€ Preprocess text (remove URLs, special chars)

     â”œâ”€ Parallel predictions:

- **TensorFlow 2.13.0**: LSTM model   â”‚  â”œâ”€ SVM: Vectorize â†’ Predict â†’ 99.5%

  - Keras API for neural networks   â”‚  â”œâ”€ LSTM: Tokenize â†’ Pad â†’ Predict â†’ 82.3%

  - Sequential LSTM architecture   â”‚  â””â”€ BERT: Tokenize â†’ Extract features â†’ Predict â†’ 91.2%

  - 87.0% accuracy on test set   â”œâ”€ Ensemble voting (majority wins)

     â”œâ”€ Calculate average confidence

- **Transformers 4.30.0**: BERT model   â””â”€ Pass to Agent 3

  - HuggingFace library

  - DistilBERT base uncased4. AGENT 3: VERDICT AGENT

  - 89.0% accuracy on test set   â”œâ”€ Analyze consensus (all agree? majority?)

     â”œâ”€ Calculate final confidence (weighted by agreement)

- **PyTorch**: BERT backend   â”œâ”€ Generate explanation:

  - Tensor operations   â”‚  â”œâ”€ AI analysis section

  - GPU acceleration (if available)   â”‚  â”œâ”€ Individual model results

   â”‚  â””â”€ NewsAPI verification (if available)

#### **NLP Libraries**   â””â”€ Return final verdict

- **NLTK 3.8**: Natural Language Processing

  - Tokenization5. RESPONSE TO USER

  - Stopword removal   â””â”€ JSON response with:

  - Text preprocessing      â”œâ”€ Verdict (TRUE/FALSE)

        â”œâ”€ Confidence (91.0%)

- **spaCy**: Advanced NLP      â”œâ”€ Explanation (human-readable)

  - Named Entity Recognition      â”œâ”€ Individual results

  - Part-of-speech tagging      â””â”€ News verification

  - Dependency parsing```



#### **External APIs**---

- **NewsAPI.org**: News fetching

  - Top headlines by country## ğŸ”Œ Integration Points

  - Search by keywords

  - Category filtering### **Between Agents**:

  - 100 requests/day (free tier)

#### **Agent 1 â†’ Agent 2**:

#### **Web Scraping**```python

- **BeautifulSoup4 4.12.2**: HTML parsing# Agent 1 prepares text and passes to Agent 2

  - Extract article contenttext_to_analyze = content or description or title

  - Clean HTML tagsresult = predictor.ensemble_predict_majority(text_to_analyze)

  - Handle various website structures```

  

- **requests 2.31.0**: HTTP client#### **Agent 2 â†’ Agent 3**:

  - Fetch web pages```python

  - Handle redirects# Agent 2 provides predictions to Agent 3

  - Timeout managementmodel_results = [

    ModelResult(

#### **Data Processing**        model_name="SVM",

- **NumPy 1.24.0**: Numerical operations        label=svm_result['prediction'].lower(),

  - Array operations        confidence=svm_result['confidence'] / 100.0,

  - Mathematical functions        model_type="traditional_ml",

          accuracy=0.995

- **pandas 2.0.0**: Data manipulation    ),

  - DataFrames for analytics    # ... LSTM and BERT results

  - CSV handling]

verdict = verdict_agent.generate_verdict(text, model_results)

#### **Commercial Features**```

- **Stripe**: Payment processing (optional)

- **JWT**: Authentication tokens#### **Agent 3 â†’ API Response**:

- **SQLite**: User data storage (optional)```python

# Agent 3 formats final response

---response = {

    'prediction': verdict['verdict'],

## 5. API Endpoints    'confidence': verdict['confidence'],

    'explanation': verdict['explanation'],

### Core Analysis Endpoints    'individual_results': ml_result['individual_results'],

    'news_api_results': news_api_results

#### `POST /analyze`}

**Analyze text for fake news**return jsonify(response)

```

**Request**:

```json### **External Integrations**:

{

  "text": "Your news article text here..."1. **NewsAPI.org**: Real-time news fetching

}2. **Web Scraping**: URL content extraction

```3. **Session Storage**: History tracking

4. **Commercial Features**: User management, usage tracking

**Response**:

```json---

{

  "prediction": "TRUE",## ğŸŒ API Endpoints

  "confidence": 95.5,

  "verdict_type": "TRUE",### **Main Flask Application** (app.py)

  "explanation": "All three ML models strongly agree...",

  "timestamp": "2025-10-18T10:30:00",#### 1. **Analyze Text**

  "model_results": {```http

    "svm": {"prediction": "TRUE", "confidence": 99.5},POST /analyze

    "lstm": {"prediction": "TRUE", "confidence": 87.0},Content-Type: application/json

    "bert": {"prediction": "TRUE", "confidence": 89.0}

  },{

  "consensus": {  "text": "Article text to analyze..."

    "agreement_level": "STRONG",}

    "votes": {"TRUE": 3, "FAKE": 0}

  }Response:

}{

```  "prediction": "TRUE",

  "confidence": 91.0,

**Usage**:  "explanation": "ğŸ¤– AI ANALYSIS: High probability...",

```python  "individual_results": {

import requests    "SVM": {"prediction": "TRUE", "confidence": 99.5},

    "LSTM": {"prediction": "TRUE", "confidence": 82.3},

response = requests.post('http://localhost:5000/analyze', json={    "BERT": {"prediction": "TRUE", "confidence": 91.2}

    'text': 'Your news text here'  },

})  "news_api_results": {...},

result = response.json()  "timestamp": "2024-10-13T12:00:00"

print(f"Prediction: {result['prediction']}")}

print(f"Confidence: {result['confidence']}%")```

```

#### 2. **Analyze URL**

---```http

POST /analyze-url

#### `POST /analyze-url`Content-Type: application/json

**Extract and analyze content from URL**

{

**Request**:  "url": "https://example.com/article"

```json}

{

  "url": "https://example.com/news-article"Response:

}{

```  "prediction": "FALSE",

  "confidence": 85.0,

**Response**:  "article_title": "Extracted Title",

```json  "article_source": "Example News",

{  "article_text": "Extracted content...",

  "prediction": "FALSE",  "explanation": "ğŸ¤– AI ANALYSIS: High probability of misinformation...",

  "confidence": 92.3,  "individual_results": {...}

  "verdict_type": "FALSE",}

  "explanation": "Two out of three models detected...",```

  "url": "https://example.com/news-article",

  "extracted_text_preview": "First 200 characters...",#### 3. **Fetch News**

  "extraction_success": true,```http

  "model_results": {...},POST /fetch-news

  "news_verification": {Content-Type: application/json

    "found": true,

    "matching_articles": [...]{

  }  "country": "us",

}  "category": "technology",

```  "page_size": 10

}

**Usage**:

```pythonResponse:

response = requests.post('http://localhost:5000/analyze-url', json={{

    'url': 'https://example.com/article'  "articles": [

})    {

result = response.json()      "title": "Article Title",

```      "description": "Article description",

      "url": "https://...",

---      "source": "News Source",

      "published_at": "2024-10-13",

#### `POST /fetch-news`      "credibility_score": 0.85,

**Fetch and analyze news from NewsAPI**      "prediction": "TRUE",

      "confidence": 91.0,

**Request**:      "individual_predictions": {...}

```json    },

{    ...

  "country": "us",  ]

  "category": "technology",}

  "page_size": 10```

}

```#### 4. **Get History**

```http

**Parameters**:GET /history

- `country` (optional): 2-letter country code (us, gb, in, etc.)

- `category` (optional): business, entertainment, general, health, science, sports, technologyResponse:

- `page_size` (optional): Number of articles (1-100, default: 20){

  "history": [

**Response**:    {

```json      "prediction": "TRUE",

{      "confidence": 91.0,

  "articles": [      "text": "Analyzed text...",

    {      "timestamp": "2024-10-13T12:00:00"

      "title": "Article Title",    },

      "description": "Article description...",    ...

      "url": "https://...",  ]

      "urlToImage": "https://...",}

      "publishedAt": "2025-10-18T...",```

      "source": {"name": "Source Name"},

      "analysis": {#### 5. **Clear History**

        "prediction": "TRUE",```http

        "confidence": 94.2,POST /clear-history

        "verdict_type": "TRUE"

      }Response:

    }{

  ],  "message": "History cleared"

  "statistics": {}

    "total": 10,```

    "true_news": 7,

    "fake_news": 2,---

    "uncertain": 1,

    "avg_confidence": 91.5## ğŸ› ï¸ Technology Stack

  }

}### **Backend**:

```- **Flask 3.x**: Web framework

- **Python 3.8+**: Programming language

**Usage**:- **NumPy**: Numerical computing

```python- **Pickle**: Model serialization

response = requests.post('http://localhost:5000/fetch-news', json={

    'country': 'us',### **Machine Learning**:

    'category': 'technology',- **scikit-learn**: SVM model, TF-IDF vectorization

    'page_size': 10- **TensorFlow/Keras**: LSTM neural network

})- **Transformers (HuggingFace)**: BERT/DistilBERT

news = response.json()- **PyTorch**: BERT model backend

```

### **Data Processing**:

---- **BeautifulSoup4**: HTML parsing (URL extraction)

- **Requests**: HTTP client

#### `GET /history`- **hashlib**: Duplicate detection

**Retrieve analysis history**- **re (regex)**: Text preprocessing



**Response**:### **External APIs**:

```json- **NewsAPI.org**: News fetching

{- **python-dotenv**: Environment configuration

  "history": [

    {### **Frontend** (not covered in detail):

      "text_preview": "First 100 chars...",- **Tailwind CSS**: Styling

      "prediction": "TRUE",- **JavaScript**: Interactivity

      "confidence": 95.5,- **Jinja2**: Template rendering

      "timestamp": "2025-10-18T10:30:00",

      "verdict_type": "TRUE"---

    }

  ],## âœ¨ Key Features

  "count": 5

}### **1. Multi-Model Ensemble**

```- Combines 3 different ML approaches

- Majority voting for robustness

---- Individual model results available

- Graceful degradation if models fail

#### `POST /clear-history`

**Clear analysis history**### **2. Real-Time News Verification**

- Fetch latest news from NewsAPI

**Response**:- Cross-reference analyzed text with trusted sources

```json- Similarity scoring

{- Best match identification

  "message": "History cleared",

  "success": true### **3. Comprehensive Analysis**

}- Sentiment analysis

```- Readability scoring

- Sensational language detection

---- Factual indicators counting

- Punctuation analysis

#### `GET /test`

**Health check endpoint**### **4. Human-Readable Output**

- Clear verdicts (TRUE/FALSE/MISLEADING/UNCERTAIN)

**Response**:- Confidence percentages

```json- Detailed explanations with emojis

{- Model-by-model breakdown

  "status": "healthy",- NewsAPI verification results

  "model_status": {

    "svm": "loaded",### **5. Advanced Features**

    "lstm": "loaded",- URL content extraction

    "bert": "loaded"- Duplicate detection

  },- Session-based history

  "news_verifier": "available",- Pagination support

  "timestamp": "2025-10-18T10:30:00"- Statistics generation

}

```---



---## ğŸ“¦ File Structure



### Commercial Endpoints (Optional)```

d:\ML Projects\FND\

#### `POST /commercial/register`â”œâ”€â”€ app.py                          # Main Flask application

Register new userâ”œâ”€â”€ config.py                       # Configuration settings

â”œâ”€â”€ .env                            # Environment variables

#### `POST /commercial/login`â”œâ”€â”€ requirements.txt                # Python dependencies

User authenticationâ”‚

â”œâ”€â”€ news_apis/

#### `GET /commercial/dashboard`â”‚   â”œâ”€â”€ __init__.py

User dashboard with analyticsâ”‚   â””â”€â”€ newsapi_client.py           # AGENT 1: NewsAPI client

â”‚

#### `POST /commercial/upgrade`â”œâ”€â”€ news_fetcher.py                 # AGENT 1: Main orchestrator

Upgrade subscription planâ”‚

â”œâ”€â”€ utils/

#### `GET /commercial/usage`â”‚   â”œâ”€â”€ __init__.py

Check API usage limitsâ”‚   â”œâ”€â”€ model_loader.py             # AGENT 2: Model loading

â”‚   â”œâ”€â”€ predictor.py                # AGENT 2: Ensemble prediction

---â”‚   â”œâ”€â”€ text_preprocessor.py        # Text cleaning

â”‚   â””â”€â”€ news_verifier.py            # NewsAPI verification

## 6. Integration Pointsâ”‚

â”œâ”€â”€ credibility_analyzer/

### Agent Integrationâ”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ credibility_analyzer.py     # AGENT 2: Analysis logic

#### **Agent 1 â†’ Agent 2 Integration**â”‚   â”œâ”€â”€ feature_extractor.py        # AGENT 2: Feature extraction

â”‚   â”œâ”€â”€ confidence_calibrator.py    # AGENT 2: Confidence scoring

```pythonâ”‚   â””â”€â”€ text_preprocessor.py        # Text preprocessing

# In app.py or news_fetcher.pyâ”‚

â”œâ”€â”€ verdict_agent/

# Agent 1: Extract and clean textâ”‚   â”œâ”€â”€ __init__.py

from news_fetcher import NewsFetcherâ”‚   â””â”€â”€ verdict_agent.py            # AGENT 3: Verdict generation

fetcher = NewsFetcher()â”‚

clean_text = fetcher._analyze_article(article)â”œâ”€â”€ models/

â”‚   â”œâ”€â”€ new_svm_model.pkl           # SVM classifier

# Pass to Agent 2â”‚   â”œâ”€â”€ new_svm_vectorizer.pkl      # TF-IDF vectorizer

from utils.predictor import UnifiedPredictorâ”‚   â”œâ”€â”€ lstm_fake_news_model.h5     # LSTM model

result = predictor.ensemble_predict_majority(clean_text['content'])â”‚   â”œâ”€â”€ lstm_tokenizer.pkl          # LSTM tokenizer

```â”‚   â””â”€â”€ bert_fake_news_model/       # BERT model directory

â”‚       â””â”€â”€ classifier.pkl          # Custom classifier

**Data Contract**:â”‚

- **Input to Agent 2**: Clean string (no HTML, normalized whitespace)â”œâ”€â”€ templates/

- **Output from Agent 2**: Dict with prediction, confidence, model_resultsâ”‚   â”œâ”€â”€ index.html                  # Main UI

â”‚   â””â”€â”€ home.html                   # Landing page

---â”‚

â”œâ”€â”€ static/

#### **Agent 2 â†’ Agent 3 Integration**â”‚   â”œâ”€â”€ css/

â”‚   â””â”€â”€ js/

```pythonâ”‚       â””â”€â”€ main.js                 # Frontend logic

# In app.pyâ”‚

â””â”€â”€ commercial/                     # Commercial features (optional)

# Agent 2: Get ML predictions    â”œâ”€â”€ auth/

ml_result = predictor.ensemble_predict_majority(text)    â”œâ”€â”€ api_limits/

    â””â”€â”€ subscriptions/

# Convert to ModelResult objects for Agent 3```

from verdict_agent.verdict_agent import VerdictAgent, ModelResult

---

model_results = []

for model_name, result in ml_result['model_results'].items():## ğŸš€ Deployment Guide

    model_results.append(ModelResult(

        model_name=model_name,### **Local Development**:

        prediction=result['prediction'],

        confidence=result['confidence'],1. **Install Dependencies**:

        raw_output=result```bash

    ))pip install -r requirements.txt

```

# Pass to Agent 3

agent = VerdictAgent()2. **Configure Environment**:

verdict = agent.generate_verdict(text, model_results)```bash

```# Create .env file

cp .env.example .env

**Data Contract**:

- **Input to Agent 3**: List of ModelResult objects# Add your NewsAPI key

- **Output from Agent 3**: Dict with verdict, explanation, confidenceNEWSAPI_KEY=your_api_key_here

```

---

3. **Start Server**:

### External Service Integration```bash

python app.py

#### **NewsAPI Integration**```



```python4. **Access Application**:

# In news_apis/newsapi_client.py```

http://localhost:5000

from newsapi import NewsApiClient```



api_key = os.getenv('NEWSAPI_KEY')### **Production Deployment**:

newsapi = NewsApiClient(api_key=api_key)

1. **Use Production Server** (Gunicorn):

# Fetch top headlines```bash

headlines = newsapi.get_top_headlines(gunicorn -w 4 -b 0.0.0.0:5000 app:app

    country='us',```

    category='technology',

    page_size=202. **Set Production Config**:

)```bash

```export FLASK_DEBUG=False

export FLASK_ENV=production

**Configuration**:```

- Set `NEWSAPI_KEY` in `.env` file

- Free tier: 100 requests/day3. **Use Reverse Proxy** (Nginx):

- Response cached for performance```nginx

server {

---    listen 80;

    server_name your-domain.com;

#### **Model Loading Integration**    

    location / {

```python        proxy_pass http://127.0.0.1:5000;

# In app.py - initialization        proxy_set_header Host $host;

        proxy_set_header X-Real-IP $remote_addr;

from utils.model_loader import ModelLoader    }

from utils.predictor import UnifiedPredictor}

```

# Load models once at startup

model_loader = ModelLoader()---

model_loader.load_all_models()  # Loads SVM, LSTM, BERT

## ğŸ“Š Performance Benchmarks

# Create predictor

predictor = UnifiedPredictor(model_loader)### **Model Accuracies**:

```- SVM: **99.5%** (best for structured features)

- LSTM: **87.0%** (good for temporal patterns)

**Model Paths** (in `config.py`):- BERT: **89.0%** (excellent context understanding)

```python- **Ensemble: ~94%** (combined strength)

SVM_MODEL_PATH = 'models/final_linear_svm.pkl'

LSTM_MODEL_PATH = 'models/lstm_best_model.h5'### **Processing Times** (average):

BERT_MODEL_PATH = 'models/bert_fake_news_model/'- Text Analysis: **0.5-1.5 seconds**

```- URL Extraction: **2-5 seconds**

- News Fetching: **2-5 seconds** (10 articles)

---- Model Loading: **5-10 seconds** (startup only)



## 7. Security & Performance### **Scalability**:

- **Concurrent Users**: 50-100 (single instance)

### Security Measures- **Articles/Day**: Unlimited (depends on NewsAPI plan)

- **Memory Usage**: ~500MB (all models loaded)

#### **Input Validation**- **CPU Usage**: Moderate (mostly during prediction)

- Maximum text length: 10,000 characters

- URL validation before fetching---

- HTML tag stripping from user input

- SQL injection prevention (parameterized queries in commercial features)## ğŸ” Security & Privacy



#### **Rate Limiting**### **Security Features**:

- NewsAPI: 100 requests/day (free tier)- âœ… API key protection (.env files, .gitignore)

- Commercial API: Per-plan limits (100-10,000 req/month)- âœ… CORS configuration (Flask-CORS)

- Session-based throttling for free users- âœ… Session management (server-side)

- âœ… Input validation

#### **API Key Protection**- âœ… Error handling (no sensitive data leaks)

- Environment variables for sensitive data

- `.env` file not in version control### **Privacy**:

- Keys never exposed in client-side code- âœ… No user data storage (session-based only)

- âœ… No tracking or analytics (by default)

#### **CORS Configuration**- âœ… Local model inference (no data sent to external AI services)

```python- âœ… NewsAPI requests are anonymous

from flask_cors import CORS

CORS(app)  # Configure for production domains---

```

## ğŸ“ˆ Future Enhancements

### Performance Optimizations

### **Planned Features**:

#### **Model Loading**

- Models loaded once at application startup1. **Agent 1 (Article Collector)**:

- Shared across all requests (global variables)   - Multi-source support (Twitter, Reddit, RSS)

- Lazy loading for optional components   - Advanced web scraping (JavaScript rendering)

   - Image and video analysis

#### **Caching**   - Social media integration

- Session-based history caching

- NewsAPI responses cached (5 minutes)2. **Agent 2 (Credibility Analyzer)**:

- Preprocessed text cached for repeated analysis   - Additional ML models (XGBoost, Random Forest)

   - Fine-tuned BERT for fake news

#### **Response Time**   - Claim extraction and verification

- Average analysis: 1-2 seconds   - Context-aware analysis

- SVM: ~0.5 seconds (fastest)

- LSTM: ~1 second3. **Agent 3 (Verdict Agent)**:

- BERT: ~1.5 seconds (slowest)   - LLM integration (GPT-4, Claude)

- Parallel execution reduces total time   - Reasoning chain visualization

   - Multi-language support

#### **Memory Management**   - Explainable AI features

- Models loaded in memory (~500MB total)

- Garbage collection for large texts4. **System-Wide**:

- Session cleanup for expired data   - User authentication

   - Dashboard analytics

---   - Batch processing

   - API rate limiting

## 8. Deployment Guide   - Mobile app



### Local Development Setup---



#### **Step 1: Clone Repository**## ğŸ“š Documentation

```powershell

git clone <repository-url>### **Agent Documentation**:

cd FND- [AGENT_1_ARTICLE_COLLECTOR.md](AGENT_1_ARTICLE_COLLECTOR.md)

```- [AGENT_2_CREDIBILITY_ANALYZER.md](AGENT_2_CREDIBILITY_ANALYZER.md)

- [AGENT_3_VERDICT_AGENT.md](AGENT_3_VERDICT_AGENT.md)

#### **Step 2: Create Virtual Environment**

```powershell### **Additional Resources**:

python -m venv venv- `README.md`: Project overview and setup

.\venv\Scripts\activate- `QUICK_FIX_README.txt`: Troubleshooting guide

```- `COMPLETE_FIX_GUIDE.txt`: Detailed fixes

- `REBUILD_SUMMARY.md`: System rebuild notes

#### **Step 3: Install Dependencies**

```powershell---

pip install -r requirements.txt

```## ğŸ¤ Contributing



#### **Step 4: Configure Environment**### **How Each Agent Can Be Extended**:

```powershell

# Copy .env.example to .env#### **Agent 1 Extensions**:

Copy-Item .env.example .env```python

# Add new data source

# Edit .env and add your API keysclass TwitterFetcher:

notepad .env    def fetch_tweets(self, query):

```        # Implementation

        pass

Required in `.env`:

```# Integrate in news_fetcher.py

NEWSAPI_KEY=your_newsapi_key_hereif source == 'twitter':

SECRET_KEY=your_secret_key_here    fetcher = TwitterFetcher()

FLASK_DEBUG=True    articles = fetcher.fetch_tweets(query)

``````



#### **Step 5: Download Models**#### **Agent 2 Extensions**:

Ensure model files are in `models/` directory:```python

- `final_linear_svm.pkl`# Add new model

- `final_vectorizer.pkl`class XGBoostPredictor:

- `lstm_best_model.h5`    def predict(self, text):

- `lstm_tokenizer.pkl`        # Implementation

- `bert_fake_news_model/` (directory with BERT files)        pass



#### **Step 6: Run Application**# Add to model_loader.py

```powershelldef load_xgboost_model(self):

python app.py    self.models['xgboost'] = load_xgboost()

``````



Application starts on `http://localhost:5000`#### **Agent 3 Extensions**:

```python

---# Add LLM reasoning

class LLMReasoning:

### Production Deployment    def explain_with_llm(self, verdict):

        # Implementation

#### **Using Gunicorn (Linux/Mac)**        pass



```bash# Integrate in verdict_agent.py

# Install gunicornllm_explanation = self.llm_client.explain(verdict)

pip install gunicorn```



# Run with 4 workers---

gunicorn -w 4 -b 0.0.0.0:5000 app:app

```## ğŸ“ Support



#### **Using Waitress (Windows)**For issues, questions, or contributions:

- GitHub Issues: [Repository URL]

```powershell- Documentation: See individual agent docs

# Install waitress- Email: [Contact Email]

pip install waitress

---

# Run server

waitress-serve --host=0.0.0.0 --port=5000 app:app## ğŸ“„ License

```

[Your License Here]

#### **Docker Deployment**

---

```dockerfile

FROM python:3.9-slim**System Version**: 1.0  

**Last Updated**: October 2025  

WORKDIR /app**Status**: Production Ready âœ…

COPY requirements.txt .

RUN pip install -r requirements.txt---



COPY . .## ğŸ‰ Summary



EXPOSE 5000The **Fake News Detection System** is a sophisticated three-agent architecture:

CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]

```1. **Agent 1** fetches and prepares news articles

2. **Agent 2** analyzes credibility with ML ensemble

```powershell3. **Agent 3** synthesizes verdicts and generates explanations

docker build -t fake-news-detector .

docker run -p 5000:5000 --env-file .env fake-news-detectorTogether, they provide:

```- **94% accuracy** with ensemble models

- **Real-time** news verification

#### **Environment Configuration**- **Human-readable** explanations

- **Comprehensive** analysis

**Production `.env`**:

```The system is **production-ready**, **scalable**, and **extensible** for future enhancements.

NEWSAPI_KEY=your_production_key

SECRET_KEY=generate_random_secure_key---

FLASK_DEBUG=False
FLASK_ENV=production
```

**Generate secure key**:
```python
import secrets
print(secrets.token_hex(32))
```

---

### Cloud Deployment

#### **Heroku**

```powershell
# Install Heroku CLI
# Create Procfile
echo "web: gunicorn app:app" > Procfile

# Deploy
heroku create fake-news-detector
heroku config:set NEWSAPI_KEY=your_key
heroku config:set SECRET_KEY=your_secret
git push heroku main
```

#### **AWS EC2**

1. Launch EC2 instance (Ubuntu 20.04)
2. SSH into instance
3. Install Python and dependencies
4. Clone repository
5. Configure environment
6. Run with Gunicorn + Nginx

#### **Google Cloud Run**

```powershell
gcloud run deploy fake-news-detector \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

---

## 9. File Structure

```
FND/
â”œâ”€â”€ app.py                          # Main Flask application (698 lines)
â”‚   â”œâ”€â”€ Route definitions
â”‚   â”œâ”€â”€ ML component initialization
â”‚   â”œâ”€â”€ Request handlers
â”‚   â””â”€â”€ Error handling
â”‚
â”œâ”€â”€ config.py                       # Configuration settings
â”‚   â”œâ”€â”€ Environment variables
â”‚   â”œâ”€â”€ Model paths
â”‚   â””â”€â”€ API keys
â”‚
â”œâ”€â”€ news_fetcher.py                 # Agent 1: Article Collector (222 lines)
â”‚   â”œâ”€â”€ NewsFetcher class
â”‚   â”œâ”€â”€ fetch_and_analyze()
â”‚   â”œâ”€â”€ _analyze_article()
â”‚   â””â”€â”€ get_statistics()
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ .env                           # Environment variables (not in git)
â”œâ”€â”€ .env.example                   # Environment template
â”‚
â”œâ”€â”€ models/                        # ML model files (~500MB)
â”‚   â”œâ”€â”€ final_linear_svm.pkl
â”‚   â”œâ”€â”€ final_vectorizer.pkl
â”‚   â”œâ”€â”€ lstm_best_model.h5
â”‚   â”œâ”€â”€ lstm_tokenizer.pkl
â”‚   â””â”€â”€ bert_fake_news_model/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”œâ”€â”€ classifier.pkl
â”‚       â””â”€â”€ vocab.txt
â”‚
â”œâ”€â”€ utils/                         # Agent 2: ML utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py            # Load all ML models (251 lines)
â”‚   â”œâ”€â”€ predictor.py               # Ensemble prediction (326 lines)
â”‚   â”œâ”€â”€ news_verifier.py           # NewsAPI verification
â”‚   â””â”€â”€ text_preprocessor.py       # Text cleaning
â”‚
â”œâ”€â”€ news_apis/                     # Agent 1: News API clients
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ newsapi_client.py          # NewsAPI integration (150 lines)
â”‚
â”œâ”€â”€ verdict_agent/                 # Agent 3: Verdict generation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ verdict_agent.py           # VerdictAgent class (207 lines)
â”‚
â”œâ”€â”€ credibility_analyzer/          # Agent 2: Advanced analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ credibility_analyzer.py    # Credibility scoring
â”‚   â”œâ”€â”€ feature_extractor.py       # Feature extraction
â”‚   â”œâ”€â”€ text_preprocessor.py       # Text preprocessing
â”‚   â””â”€â”€ confidence_calibrator.py   # Confidence calibration
â”‚
â”œâ”€â”€ commercial/                    # Commercial features (optional)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ commercial_routes.py
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ user_manager.py
â”‚   â”œâ”€â”€ api_limits/
â”‚   â”‚   â””â”€â”€ usage_tracker.py
â”‚   â””â”€â”€ subscriptions/
â”‚       â””â”€â”€ plans.py
â”‚
â”œâ”€â”€ templates/                     # HTML templates
â”‚   â”œâ”€â”€ index.html                 # Main page
â”‚   â”œâ”€â”€ analyze.html               # Analysis results
â”‚   â””â”€â”€ commercial/                # Commercial pages
â”‚
â”œâ”€â”€ static/                        # Static assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ main.js
â”‚   â””â”€â”€ images/
â”‚
â””â”€â”€ docs/                          # Documentation
    â”œâ”€â”€ SYSTEM_ARCHITECTURE.md     # This file
    â”œâ”€â”€ AGENT_1_ARTICLE_COLLECTOR.md
    â”œâ”€â”€ AGENT_2_CREDIBILITY_ANALYZER.md
    â”œâ”€â”€ AGENT_3_VERDICT_AGENT.md
    â””â”€â”€ DOCUMENTATION_INDEX.md
```

---

## 10. Configuration

### Environment Variables

**Required**:
```
NEWSAPI_KEY=your_newsapi_key          # Get from newsapi.org
SECRET_KEY=your_flask_secret_key      # Random secure string
```

**Optional**:
```
FLASK_DEBUG=True                      # Development mode
FLASK_ENV=development                 # Environment
PORT=5000                             # Server port
```

### Model Configuration

In `config.py`:
```python
class Config:
    # Model paths
    SVM_MODEL_PATH = 'models/final_linear_svm.pkl'
    SVM_VECTORIZER_PATH = 'models/final_vectorizer.pkl'
    
    LSTM_MODEL_PATH = 'models/lstm_best_model.h5'
    LSTM_TOKENIZER_PATH = 'models/lstm_tokenizer.pkl'
    
    BERT_MODEL_PATH = 'models/bert_fake_news_model/'
    
    # Model parameters
    LSTM_MAX_LENGTH = 500
    BERT_MAX_LENGTH = 512
    
    # NewsAPI
    NEWSAPI_KEY = os.getenv('NEWSAPI_KEY')
    NEWSAPI_PAGE_SIZE = 20
    NEWSAPI_CACHE_TIMEOUT = 300  # 5 minutes
```

### Application Configuration

```python
# In app.py
app.config['SECRET_KEY'] = Config.SECRET_KEY
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max request
app.config['JSON_SORT_KEYS'] = False
```

---

## Performance Benchmarks

### Model Performance

| Model | Accuracy | Precision | Recall | F1-Score | Inference Time |
|-------|----------|-----------|--------|----------|----------------|
| SVM | 99.5% | 99.3% | 99.7% | 99.5% | ~0.5s |
| LSTM | 87.0% | 85.2% | 89.1% | 87.1% | ~1.0s |
| BERT | 89.0% | 88.5% | 89.5% | 89.0% | ~1.5s |
| **Ensemble** | **94.0%** | **93.5%** | **94.5%** | **94.0%** | **~1.5s** (parallel) |

### System Performance

- **Throughput**: 40-60 requests/minute (single worker)
- **Memory Usage**: 500MB (models loaded)
- **Startup Time**: 10-15 seconds (model loading)
- **Average Response**: 1.5-2 seconds (complete analysis)

### Scalability

- **Horizontal**: Multiple Gunicorn workers
- **Vertical**: GPU acceleration for BERT/LSTM
- **Caching**: Redis for frequent queries
- **Load Balancing**: Nginx reverse proxy

---

## Future Enhancements

### Planned Features

1. **Real-time Analysis**
   - WebSocket support for streaming
   - Live news feed monitoring
   - Alert system for breaking news

2. **Advanced ML**
   - GPT-based explanation generation
   - Multi-lingual support
   - Contextual fact-checking

3. **Enhanced Verification**
   - Multiple news sources
   - Social media cross-reference
   - Expert fact-checker integration

4. **Analytics Dashboard**
   - Trend analysis
   - Source credibility tracking
   - Historical data visualization

5. **API Improvements**
   - GraphQL endpoint
   - Batch processing
   - Webhook notifications

---

## Support & Maintenance

### Monitoring

- **Health Checks**: `/test` endpoint
- **Logging**: Application and error logs
- **Metrics**: Request count, response times
- **Alerts**: Email/SMS for critical failures

### Troubleshooting

Common issues and solutions documented in:
- `QUICK_FIX_README.txt` - Quick fixes
- `COMPLETE_FIX_GUIDE.txt` - Detailed troubleshooting
- Each agent documentation - Agent-specific issues

### Updates

- **Models**: Retrain quarterly with new data
- **Dependencies**: Monthly security updates
- **NewsAPI**: Monitor API changes
- **Documentation**: Update with code changes

---

**System Version**: 1.0  
**Last Updated**: October 2025  
**Documentation**: Complete âœ…  
**Status**: Production Ready ğŸš€

---

## Quick Start Summary

```powershell
# 1. Clone and setup
git clone <repo>
cd FND
python -m venv venv
.\venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment
Copy-Item .env.example .env
# Edit .env with your NEWSAPI_KEY

# 4. Run application
python app.py

# 5. Test
curl -X POST http://localhost:5000/analyze -H "Content-Type: application/json" -d '{"text":"Your news text"}'
```

**You're ready to detect fake news!** ğŸ‰
